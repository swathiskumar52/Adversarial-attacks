# Adversarial-attacks

Adversarial examples are specially designed inputs that are supposed to fool a machine learning model resulting in a high confidence misclassification.In this project, Pretrained CNN Model(Resnet34)which is trained on Imagenet dataset is used.Experiment is implementef using Fast Gradient Sign Method(FGSM).The FGSM is a simple,effective method to generate adversial images. FGSM attack is a white-box attack with the goal of misclassification.
Performance is evaluated using Top-5 accuracy and results shows that adversial attack make fool on CNN model.Model evaluation shows that Top-5 error is very much high.ie.,the images are missclassified with high confidence.

â€‹![pic1](https://user-images.githubusercontent.com/109961297/181239941-4cb88f70-a0b9-43ea-bf35-f1df36fcd4c2.png)
